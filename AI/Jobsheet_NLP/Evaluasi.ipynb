{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nltk\n",
      "Version: 3.9.1\n",
      "Summary: Natural Language Toolkit\n",
      "Home-page: https://www.nltk.org/\n",
      "Author: NLTK Team\n",
      "Author-email: nltk.team@gmail.com\n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\users\\santo\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: click, joblib, regex, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# cek lib nltk\n",
    "!pip show nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf3af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# buat func utk import file txt 1: AWS.txt\n",
    "# membuka file txt\n",
    "# file disimpan dalam bentuk list\n",
    "def load_text(filename):\n",
    "    my_text = list()\n",
    "    with open(filename, encoding='latin-1') as f:\n",
    "        for line in islice(f, 0, None):\n",
    "            my_text.append(line)\n",
    "    return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e709c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# buat func utk tokenisasi\n",
    "def tokenize(my_text):\n",
    "    if isinstance(my_text, list):\n",
    "        my_text = [word_tokenize(sentence) for sentence in my_text]\n",
    "        flat_list = [item for sublist in my_text for item in sublist]\n",
    "        return flat_list\n",
    "    else:\n",
    "        my_text = word_tokenize(my_text)\n",
    "        return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50650460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# membuat fungsi untuk melakukan case folding\n",
    "def case_folding(list_of_words):\n",
    "    # perintah mengubah token menjadi huruf kecil:\n",
    "    list_of_words = [word.lower() for word in list_of_words]\n",
    "    # perintah menghapus angka\n",
    "    list_of_words = [re.sub(r\"\\d+\", \"\", word) for word in list_of_words]\n",
    "    # perintah menghapus tanda baca\n",
    "    list_of_words = [word.translate(str.maketrans(\"\",\"\",string.punctuation)) for word in list_of_words]\n",
    "    # perintah menghapus spasi di depan dan dibelakang teks\n",
    "    list_of_words = [word.strip() for word in list_of_words]\n",
    "    # perintah menghapus kata yang memiliki jumlah huruf <= 2\n",
    "    list_of_words = [word for word in list_of_words if len(word)>2]\n",
    "    \n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477ad98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengimpor package stopwords dari NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# membuat fungsi melakukan filtering tokens\n",
    "# stopwords yang digunakan diambil dari NLTK\n",
    "# untuk bahasa Indonesia\n",
    "def filtering(list_of_words):\n",
    "    # load stopwords:\n",
    "    stops = stopwords.words('indonesian')\n",
    "    # remove stopwords:\n",
    "    list_of_words = [word for word in list_of_words if word not in stops]\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bcd34dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install Sastrawi jika belum terpasang\n",
    "%pip install Sastrawi\n",
    "\n",
    "# perintah mengimpor package StemmerFactory dari Python Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# membuat fungsi melakukan stemming token\n",
    "def stemming(list_of_words):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    list_of_words = [stemmer.stem(word) for word in list_of_words]\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052ef442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat fungsi melakukan text preprocessing\n",
    "# memanggil fungsi-fungsi yang sudah dibuat diatas\n",
    "def preprocessing(file):\n",
    "    tokens_file = tokenize(file)\n",
    "    tokens_file = case_folding(tokens_file)\n",
    "    tokens_file = filtering(tokens_file)\n",
    "    tokens_file = stemming(tokens_file)\n",
    "    return tokens_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75249612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['max', 'verstappen', 'dominasi', 'menang', 'grand', 'prix', 'emiliaromagna', 'tekan', 'lando', 'norris', 'lomba', 'balap', 'red', 'bull', 'hasil', 'tahan', 'posisi', 'depan', 'garis', 'finis', 'menang', 'kokoh', 'posisi', 'puncak', 'klasemen']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file AWS\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('News1.txt')\n",
    "dok1 = preprocessing(file)\n",
    "print(dok1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b1cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tim', 'mclaren', 'umum', 'paket', 'upgrade', 'aerodinamika', 'jelang', 'grand', 'prix', 'monaco', 'tingkat', 'fokus', 'efisiensi', 'downforce', 'tikung', 'lambat', 'lando', 'norris', 'optimis', 'ubah', 'bawa', 'tingkat', 'performa', 'signifikan', 'sirkuit', 'jalan', 'raya', 'legendaris']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file Wikipedia\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('News2.txt')\n",
    "dok2 = preprocessing(file)\n",
    "print(dok2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f08a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ferrari', 'kembang', 'mesin', 'musim', 'klaim', 'efisien', 'tenaga', 'fokus', 'utama', 'kurang', 'konsumsi', 'bahan', 'bakar', 'tingkat', 'daya', 'tahan', 'direktur', 'teknis', 'ferrari', 'sebut', 'mesin', 'strategi', 'jangka', 'saing', 'papan']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file IBM\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('News3.txt')\n",
    "dok3 = preprocessing(file)\n",
    "print(dok3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76642800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lewis', 'hamilton', 'komentar', 'regulasi', 'mesin', 'laku', 'musim', 'turut', 'ubah', 'ubah', 'dinamika', 'balap', 'drastis', 'harus', 'tim', 'adaptasi', 'cepat', 'tekan', 'lanjut', 'kembang', 'teknologi']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file IBM\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('News4.txt')\n",
    "dok4 = preprocessing(file)\n",
    "print(dok4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1eba50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'bull', 'racing', 'selidik', 'fia', 'kait', 'potensi', 'langgar', 'batas', 'anggar', 'musim', 'bukti', 'langgar', 'tim', 'kena', 'sanksi', 'kurang', 'kembang', 'wind', 'tunnel', 'penalti', 'poin', 'red', 'bull', 'ban', 'tuduh', 'sesuai', 'regulasi']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file IBM\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('News5.txt')\n",
    "dok5 = preprocessing(file)\n",
    "print(dok5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4c02033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jaring', 'saraf', 'tiru']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk melakukan preprocessing\n",
    "# terhadap query\n",
    "# dengan memanggil fungsi preprocessing\n",
    "query = \"jaringan saraf tiruan\"\n",
    "Q = preprocessing(query)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe6f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# membuat fungsi untuk menentukan kata unik\n",
    "def cek_unique_words(unique_words, my_list):\n",
    "    # membuat list kata-kata unik\n",
    "    for item in my_list:\n",
    "        if item not in unique_words:\n",
    "            unique_words.append(item)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40830c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perintah untuk menggabungkan dataset\n",
    "documents = [dok1, dok2, dok3, dok4, dok5, Q]\n",
    "\n",
    "# perintah untuk menentukan kata unik dari dataset\n",
    "# dengan memanggil fungsi cek_unique_words\n",
    "my_unique_words = []\n",
    "for doc in documents:\n",
    "    my_unique_words = cek_unique_words(my_unique_words, doc)\n",
    "my_unique_words.sort()\n",
    "\n",
    "# fungsi untuk menghitung frekuensi kemunculan kata\n",
    "def count_freq(unique_words, my_list):\n",
    "    counts = []\n",
    "    for word in unique_words:\n",
    "        count = 0\n",
    "        for i in my_list:\n",
    "            if word == i:\n",
    "                count += 1\n",
    "        counts.append(count)\n",
    "    return counts\n",
    "\n",
    "# fungsi untuk menghitung term frequensi/TF\n",
    "def count_TF(unique_words, my_list):\n",
    "    count = count_freq(unique_words, my_list)\n",
    "    sum_count = sum(count)\n",
    "    TF = [x / sum_count for x in count]\n",
    "    return TF\n",
    "\n",
    "# perintah untuk menghitung TF dari dataset\n",
    "# dengan memanggil fungsi count_TF\n",
    "tf_dok1 = count_TF(my_unique_words, dok1)\n",
    "tf_dok2 = count_TF(my_unique_words, dok2)\n",
    "tf_dok3 = count_TF(my_unique_words, dok3)\n",
    "tf_dok4 = count_TF(my_unique_words, dok4)\n",
    "tf_dok5 = count_TF(my_unique_words, dok5)\n",
    "tf_Q = count_TF(my_unique_words, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df3a501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# membuat fungsi untuk menghitung invers document frequency/IDF\n",
    "def count_IDF(unique_words, documents, n_document):\n",
    "    # perintah menghitung jumlah dokumen yang memuat sebuah kata/DF\n",
    "    # kemudian menghitung IDF dari DF setiap kata\n",
    "    count = []\n",
    "    for word in unique_words:\n",
    "        df = 0\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                df = df + 1\n",
    "            else:\n",
    "                df = df\n",
    "        idf = math.log(n_document/df)\n",
    "        count.append(idf)\n",
    "    return count\n",
    "\n",
    "# perintah untuk menghitung IDF dari dataset\n",
    "# dengan memanggil fungsi count_IDF\n",
    "IDF = count_IDF(my_unique_words, documents, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be36de93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>NEWS1</th>\n",
       "      <th>NEWS2</th>\n",
       "      <th>NEWS3</th>\n",
       "      <th>NEWS4</th>\n",
       "      <th>NEWS5</th>\n",
       "      <th>Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaptasi</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.085322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aerodinamika</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.063991</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anggar</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bahan</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bakar</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>umum</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.063991</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>upgrade</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.063991</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>utama</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>verstappen</td>\n",
       "      <td>0.07167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>wind</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word    NEWS1     NEWS2    NEWS3     NEWS4     NEWS5  Query\n",
       "0       adaptasi  0.00000  0.000000  0.00000  0.085322  0.000000    0.0\n",
       "1   aerodinamika  0.00000  0.063991  0.00000  0.000000  0.000000    0.0\n",
       "2         anggar  0.00000  0.000000  0.00000  0.000000  0.063991    0.0\n",
       "3          bahan  0.00000  0.000000  0.07167  0.000000  0.000000    0.0\n",
       "4          bakar  0.00000  0.000000  0.07167  0.000000  0.000000    0.0\n",
       "..           ...      ...       ...      ...       ...       ...    ...\n",
       "95          umum  0.00000  0.063991  0.00000  0.000000  0.000000    0.0\n",
       "96       upgrade  0.00000  0.063991  0.00000  0.000000  0.000000    0.0\n",
       "97         utama  0.00000  0.000000  0.07167  0.000000  0.000000    0.0\n",
       "98    verstappen  0.07167  0.000000  0.00000  0.000000  0.000000    0.0\n",
       "99          wind  0.00000  0.000000  0.00000  0.000000  0.063991    0.0\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perintah untuk menghitung TF IDF dari dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TF_IDF1 = np.array(tf_dok1) * np.array(IDF)\n",
    "TF_IDF2 = np.array(tf_dok2) * np.array(IDF)\n",
    "TF_IDF3 = np.array(tf_dok3) * np.array(IDF)\n",
    "TF_IDF4 = np.array(tf_dok4) * np.array(IDF)\n",
    "TF_IDF5 = np.array(tf_dok5) * np.array(IDF)\n",
    "TF_IDFQ = np.array(tf_Q) * np.array(IDF)\n",
    "\n",
    "# perintah untuk membuat dataframe yang merangkum hasil diatas\n",
    "df_result = pd.DataFrame(\n",
    "    {\"word\": my_unique_words, \"NEWS1\": TF_IDF1, \"NEWS2\": TF_IDF2, \"NEWS3\": TF_IDF3, \"NEWS4\": TF_IDF4, \"NEWS5\": TF_IDF5, \"Query\": TF_IDFQ}\n",
    ")\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e439bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\santo\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk install Scikit-Learn\n",
    "%pip install scikit-learn\n",
    "\n",
    "# perintah menggabungkan dataset dan query menjadi satu dataframe\n",
    "df = pd.DataFrame(\n",
    "    {'Document': [0,1,2,3,4,5],\n",
    "     'Term': [dok1, dok2, dok3, dok4, dok5, Q]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83f6c2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    max verstappen dominasi menang grand prix emil...\n",
       "1    tim mclaren umum paket upgrade aerodinamika je...\n",
       "2    ferrari kembang mesin musim klaim efisien tena...\n",
       "3    lewis hamilton komentar regulasi mesin laku mu...\n",
       "4    red bull racing selidik fia kait potensi langg...\n",
       "5                                    jaring saraf tiru\n",
       "Name: Term, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mengimpor library ast\n",
    "# https://docs.python.org/3/library/ast.html\n",
    "import ast\n",
    "\n",
    "# fungsi menggabungkan semua token menjadi satu dokumen tunggal\n",
    "def join_text_list(texts):\n",
    "    return ' '.join([text for text in texts])\n",
    "\n",
    "# perintah memanggil fungsi join_text_list\n",
    "df_join = df[\"Term\"].apply(join_text_list)\n",
    "# perintah menggabungkan ketiga dokumen dalam satu dataframe\n",
    "df_join = df_join[:6]\n",
    "df_join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "731d63ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Word     NEWS1     NEWS2     NEWS3     NEWS4  Query\n",
      "0       adaptasi  0.000000  0.000000  0.000000  0.000000    0.0\n",
      "1   aerodinamika  0.000000  0.195353  0.000000  0.000000    0.0\n",
      "2         anggar  0.000000  0.000000  0.000000  0.185178    0.0\n",
      "3          bahan  0.000000  0.000000  0.198662  0.000000    0.0\n",
      "4          bakar  0.000000  0.000000  0.198662  0.000000    0.0\n",
      "..           ...       ...       ...       ...       ...    ...\n",
      "95          umum  0.000000  0.195353  0.000000  0.000000    0.0\n",
      "96       upgrade  0.000000  0.195353  0.000000  0.000000    0.0\n",
      "97         utama  0.000000  0.000000  0.198662  0.000000    0.0\n",
      "98    verstappen  0.195921  0.000000  0.000000  0.000000    0.0\n",
      "99          wind  0.000000  0.000000  0.000000  0.185178    0.0\n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# mengimpor package TfidfVectorizer\n",
    "# untuk menghitung TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "vec = TfidfVectorizer()\n",
    "matrix = vec.fit_transform(df_join)\n",
    "\n",
    "# perintah mengubah sparse matrix hasil fit transform\n",
    "# menjadi array\n",
    "matrix = matrix.toarray()\n",
    "\n",
    "# perintah menentukan kata-kata unik\n",
    "name = vec.get_feature_names_out()\n",
    "\n",
    "# perintah menggabungkan nama dan nilai matrix dalam data frame\n",
    "tfidf_vector = pd.DataFrame({'Word': name, 'NEWS1':matrix[0], 'NEWS2':matrix[1], 'NEWS3': matrix[2], 'NEWS4':matrix[3], 'NEWS4':matrix[4], 'Query':matrix[5]})\n",
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f12d06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Query dan News1: 0.0000\n",
      "Cosine Similarity Query dan News2: 0.0000\n",
      "Cosine Similarity Query dan News3: 0.0000\n",
      "Cosine Similarity Query dan News4: 0.0000\n",
      "Cosine Similarity Query dan News5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# membuat fungsi untuk menghitung cosine similarity\n",
    "def count_cosine(query, doc):\n",
    "    xy = [x * y for x, y in zip(query, doc)]\n",
    "    sum_xy = sum(xy)\n",
    "    x2 = [math.pow(x,2) for x in query]\n",
    "    sum_x2 = math.sqrt(sum(x2))\n",
    "    y2 = [math.pow(y,2) for y in doc]\n",
    "    sum_y2 = math.sqrt(sum(y2))\n",
    "    cosine_xy = sum_xy / (sum_x2 * sum_y2)\n",
    "    return cosine_xy\n",
    "\n",
    "# perintah untuk menghitung cosine similarity\n",
    "# antara query dengan dataset\n",
    "# dengan memanggil fungsi count_cosine\n",
    "cosine_Q_dok1 = count_cosine(TF_IDFQ, TF_IDF1)\n",
    "cosine_Q_dok2 = count_cosine(TF_IDFQ, TF_IDF2)\n",
    "cosine_Q_dok3 = count_cosine(TF_IDFQ, TF_IDF3)\n",
    "cosine_Q_dok4 = count_cosine(TF_IDFQ, TF_IDF4)\n",
    "cosine_Q_dok5 = count_cosine(TF_IDFQ, TF_IDF5)\n",
    "\n",
    "# perintah untuk menampilkan hasil cosine\n",
    "print(f\"Cosine Similarity Query dan News1: {cosine_Q_dok1:.4f}\")\n",
    "print(f\"Cosine Similarity Query dan News2: {cosine_Q_dok2:.4f}\")\n",
    "print(f\"Cosine Similarity Query dan News3: {cosine_Q_dok3:.4f}\")\n",
    "print(f\"Cosine Similarity Query dan News4: {cosine_Q_dok4:.4f}\")\n",
    "print(f\"Cosine Similarity Query dan News5: {cosine_Q_dok5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "769ddc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity News1 dan News2: 0.0650\n",
      "Cosine Similarity News1 dan News3: 0.0165\n",
      "Cosine Similarity News1 dan News4: 0.0396\n",
      "Cosine Similarity News1 dan News5: 0.0619\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk menghitung cosine similarity\n",
    "# antara semua dokumen dalam dataset\n",
    "# selain query\n",
    "# dengan memanggil fungsi count_cosine\n",
    "cosine_Q_dok2 = count_cosine(TF_IDF1, TF_IDF2)\n",
    "cosine_Q_dok3 = count_cosine(TF_IDF1, TF_IDF3)\n",
    "cosine_Q_dok4 = count_cosine(TF_IDF1, TF_IDF4)\n",
    "cosine_Q_dok5 = count_cosine(TF_IDF1, TF_IDF5)\n",
    "\n",
    "print(f\"Cosine Similarity News1 dan News2: {cosine_Q_dok2:.4f}\")\n",
    "print(f\"Cosine Similarity News1 dan News3: {cosine_Q_dok3:.4f}\")\n",
    "print(f\"Cosine Similarity News1 dan News4: {cosine_Q_dok4:.4f}\")\n",
    "print(f\"Cosine Similarity News1 dan News5: {cosine_Q_dok5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b23b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity News1 dan News2: 0.0650\n",
      "Cosine Similarity News1 dan News3: 0.0165\n",
      "Cosine Similarity News1 dan News4: 0.0396\n",
      "Cosine Similarity News1 dan News5: 0.0619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Ubah semua vektor TF-IDF menjadi bentuk (1, n)\n",
    "TF_IDF1 = TF_IDF1.reshape(1, -1)\n",
    "TF_IDF2 = TF_IDF2.reshape(1, -1)\n",
    "TF_IDF3 = TF_IDF3.reshape(1, -1)\n",
    "TF_IDF4 = TF_IDF4.reshape(1, -1)\n",
    "TF_IDF5 = TF_IDF5.reshape(1, -1)\n",
    "\n",
    "# Hitung cosine similarity\n",
    "sim_2 = cosine_similarity(TF_IDF1, TF_IDF2)\n",
    "sim_3 = cosine_similarity(TF_IDF1, TF_IDF3)\n",
    "sim_4 = cosine_similarity(TF_IDF1, TF_IDF4)\n",
    "sim_5 = cosine_similarity(TF_IDF1, TF_IDF5)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Cosine Similarity News1 dan News2: {sim_2[0][0]:.4f}\")\n",
    "print(f\"Cosine Similarity News1 dan News3: {sim_3[0][0]:.4f}\")\n",
    "print(f\"Cosine Similarity News1 dan News4: {sim_4[0][0]:.4f}\")\n",
    "print(f\"Cosine Similarity News1 dan News5: {sim_5[0][0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
