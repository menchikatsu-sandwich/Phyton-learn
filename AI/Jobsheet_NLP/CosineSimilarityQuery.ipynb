{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8af4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nltk\n",
      "Version: 3.9.1\n",
      "Summary: Natural Language Toolkit\n",
      "Home-page: https://www.nltk.org/\n",
      "Author: NLTK Team\n",
      "Author-email: nltk.team@gmail.com\n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\users\\santo\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: click, joblib, regex, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# cek lib nltk\n",
    "!pip show nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66319154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import lib nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a3cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# buat func utk import file txt 1: AWS.txt\n",
    "# membuka file txt\n",
    "# file disimpan dalam bentuk list\n",
    "def load_text(filename):\n",
    "    my_text = list()\n",
    "    with open(filename, encoding='latin-1') as f:\n",
    "        for line in islice(f, 0, None):\n",
    "            my_text.append(line)\n",
    "    return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28302056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# buat func utk tokenisasi\n",
    "def tokenize(my_text):\n",
    "    if isinstance(my_text, list):\n",
    "        my_text = [word_tokenize(sentence) for sentence in my_text]\n",
    "        flat_list = [item for sublist in my_text for item in sublist]\n",
    "        return flat_list\n",
    "    else:\n",
    "        my_text = word_tokenize(my_text)\n",
    "        return my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "664b5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# membuat fungsi untuk melakukan case folding\n",
    "def case_folding(list_of_words):\n",
    "    # perintah mengubah token menjadi huruf kecil:\n",
    "    list_of_words = [word.lower() for word in list_of_words]\n",
    "    # perintah menghapus angka\n",
    "    list_of_words = [re.sub(r\"\\d+\", \"\", word) for word in list_of_words]\n",
    "    # perintah menghapus tanda baca\n",
    "    list_of_words = [word.translate(str.maketrans(\"\",\"\",string.punctuation)) for word in list_of_words]\n",
    "    # perintah menghapus spasi di depan dan dibelakang teks\n",
    "    list_of_words = [word.strip() for word in list_of_words]\n",
    "    # perintah menghapus kata yang memiliki jumlah huruf <= 2\n",
    "    list_of_words = [word for word in list_of_words if len(word)>2]\n",
    "    \n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a58f419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengimpor package stopwords dari NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# membuat fungsi melakukan filtering tokens\n",
    "# stopwords yang digunakan diambil dari NLTK\n",
    "# untuk bahasa Indonesia\n",
    "def filtering(list_of_words):\n",
    "    # load stopwords:\n",
    "    stops = stopwords.words('indonesian')\n",
    "    # remove stopwords:\n",
    "    list_of_words = [word for word in list_of_words if word not in stops]\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e98c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install Sastrawi jika belum terpasang\n",
    "%pip install Sastrawi\n",
    "\n",
    "# perintah mengimpor package StemmerFactory dari Python Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# membuat fungsi melakukan stemming token\n",
    "def stemming(list_of_words):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    list_of_words = [stemmer.stem(word) for word in list_of_words]\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e70a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat fungsi melakukan text preprocessing\n",
    "# memanggil fungsi-fungsi yang sudah dibuat diatas\n",
    "def preprocessing(file):\n",
    "    tokens_file = tokenize(file)\n",
    "    tokens_file = case_folding(tokens_file)\n",
    "    tokens_file = filtering(tokens_file)\n",
    "    tokens_file = stemming(tokens_file)\n",
    "    return tokens_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9189f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cerdas', 'buat', 'cerdas', 'buat', 'bidang', 'ilmu', 'komputer', 'khusus', 'pecah', 'kognitif', 'kait', 'cerdas', 'manusia', 'ajar', 'cipta', 'kenal', 'gambar', 'organisasi', 'modern', 'kumpul', 'data', 'agam', 'sumber', 'sensor', 'pintar', 'konten', 'buat', 'manusia', 'alat', 'pantau', 'log', 'sistem', 'tuju', 'cipta', 'sistem', 'ajar', 'mandiri', 'oleh', 'makna', 'data', 'terap', 'tahu', 'pecah', 'layak', 'manusia', 'teknologi', 'respons', 'cakap', 'manusia', 'makna', 'gambar', 'teks', 'asli', 'putus', 'dasar', 'input', 'data', 'nyata', 'organisasi', 'integrasi', 'mampu', 'aplikasi', 'optimal', 'proses', 'bisnis', 'tingkat', 'alam', 'langgan', 'cepat', 'inovasi']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file AWS\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('AWS.txt')\n",
    "dok1 = preprocessing(file)\n",
    "print(dok1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a150456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cerdas', 'buat', 'akal', 'imitasi', 'inggris', 'artificial', 'intelligence', 'cerdas', 'sistem', 'atur', 'konteks', 'ilmiah', 'definisi', 'cerdas', 'entitas', 'ilmiah', 'andreas', 'kaplan', 'michael', 'haenlein', 'definisi', 'cerdas', 'buat', 'mampu', 'sistem', 'tafsir', 'data', 'eksternal', 'ajar', 'data', 'ajar', 'capai', 'tuju', 'tugas', 'adaptasi', 'fleksibel', 'sistem', 'anggap', 'komputer', 'cerdas', 'cipta', 'masuk', 'komputer', 'kerja', 'manusia', 'bidang', 'cerdas', 'buat', 'sistem', 'pakar', 'main', 'komputer', 'logika', 'kabur', 'jaring', 'saraf', 'tiru', 'robotika', 'teknis', 'cerdas', 'buat', 'model', 'statistik', 'ambil', 'putus', 'menggeneralisir', 'karakteristik', 'objek', 'bas', 'data', 'pasang', 'perangkat', 'elektronik']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file Wikipedia\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('Wikipedia.txt')\n",
    "dok2 = preprocessing(file)\n",
    "print(dok2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d132517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['definisi', 'cerdas', 'buat', 'muncul', 'dekade', 'john', 'mccarthy', 'tawar', 'definisi', 'makalah', 'taut', 'ibmcom', 'ilmu', 'tahu', 'teknik', 'mesin', 'cerdas', 'program', 'komputer', 'cerdas', 'kait', 'tugas', 'komputer', 'paham', 'cerdas', 'manusia', 'batas', 'metode', 'amat', 'biologis', 'dekade', 'definisi', 'lahir', 'cakap', 'cerdas', 'buat', 'tanda', 'karya', 'alan', 'turing', 'computing', 'machinery', 'and', 'intelligence', 'taut', 'ibmcom', 'terbit', 'makalah', 'turing', 'ilmu', 'komputer', 'aju', 'dapat', 'mesin', 'pikir', 'tawar', 'tes', 'kenal', 'sebut', 'turing', 'test', 'interogator', 'manusia', 'coba', 'beda', 'respons', 'teks', 'komputer', 'manusia', 'tes', 'alami', 'awas', 'terbit', 'tes', 'sejarah', 'konsep', 'filsafat', 'ideide', 'putar', 'linguistik', 'stuart', 'russell', 'peter', 'norvig', 'lanjut', 'terbit', 'artificial', 'intelligence', 'modern', 'approach', 'taut', 'ibmcom', 'salah', 'buku', 'teks', 'muka', 'studi', 'dalam', 'ajar', 'tuju', 'potensial', 'definisi', 'beda', 'sistem', 'komputer', 'dasar', 'rasionalitas', 'pikir', 'tindak', 'dekat', 'manusia', 'sistem', 'pikir', 'manusia', 'sistem', 'tindak', 'manusia', 'dekat', 'ideal', 'sistem', 'pikir', 'rasional', 'sistem', 'tindak', 'rasional', 'definisi', 'alan', 'turing', 'masuk', 'kategori', 'sistem', 'tindak', 'manusia', 'sederhana', 'cerdas', 'buat', 'bidang', 'gabung', 'ilmu', 'komputer', 'kumpul', 'data', 'kuat', 'pecah', 'bidang', 'cakup', 'subbidang', 'ajar', 'mesin', 'ajar', 'dalam', 'disebutsebut', 'hubung', 'cerdas', 'buat', 'disiplin', 'ilmu', 'algoritme', 'usaha', 'cipta', 'sistem', 'pakar', 'prediksi', 'klasifikasi', 'dasar', 'data', 'masuk']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk membaca isi file IBM\n",
    "# dengan memanggil fungsi load_text\n",
    "# kemudian melakukan preprocessing\n",
    "# dengan memanggil fungsi preprocessing\n",
    "file = load_text('IBM.txt')\n",
    "dok3 = preprocessing(file)\n",
    "print(dok3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24755eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jaring', 'saraf', 'tiru']\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk melakukan preprocessing\n",
    "# terhadap query\n",
    "# dengan memanggil fungsi preprocessing\n",
    "query = \"jaringan saraf tiruan\"\n",
    "Q = preprocessing(query)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcb3e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# membuat fungsi untuk menentukan kata unik\n",
    "def cek_unique_words(unique_words, my_list):\n",
    "    # membuat list kata-kata unik\n",
    "    for item in my_list:\n",
    "        if item not in unique_words:\n",
    "            unique_words.append(item)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1acb737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perintah untuk menggabungkan dataset\n",
    "documents = [dok1, dok2, dok3, Q]\n",
    "\n",
    "# perintah untuk menentukan kata unik dari dataset\n",
    "# dengan memanggil fungsi cek_unique_words\n",
    "my_unique_words = []\n",
    "for doc in documents:\n",
    "    my_unique_words = cek_unique_words(my_unique_words, doc)\n",
    "my_unique_words.sort()\n",
    "\n",
    "# fungsi untuk menghitung frekuensi kemunculan kata\n",
    "def count_freq(unique_words, my_list):\n",
    "    counts = []\n",
    "    for word in unique_words:\n",
    "        count = 0\n",
    "        for i in my_list:\n",
    "            if word == i:\n",
    "                count += 1\n",
    "        counts.append(count)\n",
    "    return counts\n",
    "\n",
    "# fungsi untuk menghitung term frequensi/TF\n",
    "def count_TF(unique_words, my_list):\n",
    "    count = count_freq(unique_words, my_list)\n",
    "    sum_count = sum(count)\n",
    "    TF = [x / sum_count for x in count]\n",
    "    return TF\n",
    "\n",
    "# perintah untuk menghitung TF dari dataset\n",
    "# dengan memanggil fungsi count_TF\n",
    "tf_dok1 = count_TF(my_unique_words, dok1)\n",
    "tf_dok2 = count_TF(my_unique_words, dok2)\n",
    "tf_dok3 = count_TF(my_unique_words, dok3)\n",
    "tf_Q = count_TF(my_unique_words, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a453e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# membuat fungsi untuk menghitung invers document frequency/IDF\n",
    "def count_IDF(unique_words, documents, n_document):\n",
    "    # perintah menghitung jumlah dokumen yang memuat sebuah kata/DF\n",
    "    # kemudian menghitung IDF dari DF setiap kata\n",
    "    count = []\n",
    "    for word in unique_words:\n",
    "        df = 0\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                df = df + 1\n",
    "            else:\n",
    "                df = df\n",
    "        idf = math.log(n_document/df)\n",
    "        count.append(idf)\n",
    "    return count\n",
    "\n",
    "# perintah untuk menghitung IDF dari dataset\n",
    "# dengan memanggil fungsi count_IDF\n",
    "IDF = count_IDF(my_unique_words, documents, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f7f1e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>AWS</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>IBM</th>\n",
       "      <th>Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaptasi</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agam</td>\n",
       "      <td>0.019804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ajar</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aju</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>tiru</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>tugas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009495</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>tuju</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>turing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>usaha</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word       AWS  Wikipedia       IBM     Query\n",
       "0    adaptasi  0.000000   0.018990  0.000000  0.000000\n",
       "1        agam  0.019804   0.000000  0.000000  0.000000\n",
       "2        ajar  0.008219   0.007882  0.005168  0.000000\n",
       "3         aju  0.000000   0.000000  0.008301  0.000000\n",
       "4        akal  0.000000   0.018990  0.000000  0.000000\n",
       "..        ...       ...        ...       ...       ...\n",
       "163      tiru  0.000000   0.009495  0.000000  0.231049\n",
       "164     tugas  0.000000   0.009495  0.004151  0.000000\n",
       "165      tuju  0.004110   0.003941  0.001723  0.000000\n",
       "166    turing  0.000000   0.000000  0.033205  0.000000\n",
       "167     usaha  0.000000   0.000000  0.008301  0.000000\n",
       "\n",
       "[168 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perintah untuk menghitung TF IDF dari dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TF_IDF1 = np.array(tf_dok1) * np.array(IDF)\n",
    "TF_IDF2 = np.array(tf_dok2) * np.array(IDF)\n",
    "TF_IDF3 = np.array(tf_dok3) * np.array(IDF)\n",
    "TF_IDFQ = np.array(tf_Q) * np.array(IDF)\n",
    "\n",
    "# perintah untuk membuat dataframe yang merangkum hasil diatas\n",
    "df_result = pd.DataFrame(\n",
    "    {\"word\": my_unique_words, \"AWS\": TF_IDF1, \"Wikipedia\": TF_IDF2, \"IBM\": TF_IDF3, \"Query\": TF_IDFQ}\n",
    ")\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2d6e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\santo\\anaconda3\\envs\\dip\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\santo\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.5.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk install Scikit-Learn\n",
    "%pip install scikit-learn\n",
    "\n",
    "# perintah menggabungkan dataset dan query menjadi satu dataframe\n",
    "df = pd.DataFrame(\n",
    "    {'Document': [0,1,2,3],\n",
    "     'Term': [dok1, dok2, dok3, Q]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2022971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cerdas buat cerdas buat bidang ilmu komputer khusus pecah kognitif kait cerdas manusia ajar cipta kenal gambar organisasi modern kumpul data agam sumber sensor pintar konten buat manusia alat pantau log sistem tuju cipta sistem ajar mandiri oleh makna data terap tahu pecah layak manusia teknologi respons cakap manusia makna gambar teks asli putus dasar input data nyata organisasi integrasi mampu aplikasi optimal proses bisnis tingkat alam langgan cepat inovasi',\n",
       " 'cerdas buat akal imitasi inggris artificial intelligence cerdas sistem atur konteks ilmiah definisi cerdas entitas ilmiah andreas kaplan michael haenlein definisi cerdas buat mampu sistem tafsir data eksternal ajar data ajar capai tuju tugas adaptasi fleksibel sistem anggap komputer cerdas cipta masuk komputer kerja manusia bidang cerdas buat sistem pakar main komputer logika kabur jaring saraf tiru robotika teknis cerdas buat model statistik ambil putus menggeneralisir karakteristik objek bas data pasang perangkat elektronik',\n",
       " 'definisi cerdas buat muncul dekade john mccarthy tawar definisi makalah taut ibmcom ilmu tahu teknik mesin cerdas program komputer cerdas kait tugas komputer paham cerdas manusia batas metode amat biologis dekade definisi lahir cakap cerdas buat tanda karya alan turing computing machinery and intelligence taut ibmcom terbit makalah turing ilmu komputer aju dapat mesin pikir tawar tes kenal sebut turing test interogator manusia coba beda respons teks komputer manusia tes alami awas terbit tes sejarah konsep filsafat ideide putar linguistik stuart russell peter norvig lanjut terbit artificial intelligence modern approach taut ibmcom salah buku teks muka studi dalam ajar tuju potensial definisi beda sistem komputer dasar rasionalitas pikir tindak dekat manusia sistem pikir manusia sistem tindak manusia dekat ideal sistem pikir rasional sistem tindak rasional definisi alan turing masuk kategori sistem tindak manusia sederhana cerdas buat bidang gabung ilmu komputer kumpul data kuat pecah bidang cakup subbidang ajar mesin ajar dalam disebutsebut hubung cerdas buat disiplin ilmu algoritme usaha cipta sistem pakar prediksi klasifikasi dasar data masuk',\n",
       " 'jaring saraf tiru']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mengimpor library ast\n",
    "# https://docs.python.org/3/library/ast.html\n",
    "import ast\n",
    "\n",
    "# fungsi menggabungkan semua token menjadi satu dokumen tunggal\n",
    "def join_text_list(texts):\n",
    "    return ' '.join([text for text in texts])\n",
    "\n",
    "# perintah memanggil fungsi join_text_list\n",
    "df_join = df[\"Term\"].apply(join_text_list)\n",
    "# perintah menggabungkan ketiga dokumen dalam satu dataframe\n",
    "df_join = [df_join[0], df_join[1], df_join[2], df_join[3]]\n",
    "df_join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a091c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word       AWS  Wikipedia       IBM    Query\n",
      "0    adaptasi  0.000000   0.106422  0.000000  0.00000\n",
      "1        agam  0.116583   0.000000  0.000000  0.00000\n",
      "2        ajar  0.148826   0.135856  0.108393  0.00000\n",
      "3         aju  0.000000   0.000000  0.056606  0.00000\n",
      "4        akal  0.000000   0.106422  0.000000  0.00000\n",
      "..        ...       ...        ...       ...      ...\n",
      "163      tiru  0.000000   0.083904  0.000000  0.57735\n",
      "164     tugas  0.000000   0.083904  0.044629  0.00000\n",
      "165      tuju  0.074413   0.067928  0.036131  0.00000\n",
      "166    turing  0.000000   0.000000  0.226425  0.00000\n",
      "167     usaha  0.000000   0.000000  0.056606  0.00000\n",
      "\n",
      "[168 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# mengimpor package TfidfVectorizer\n",
    "# untuk menghitung TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "vec = TfidfVectorizer()\n",
    "matrix = vec.fit_transform(df_join)\n",
    "\n",
    "# perintah mengubah sparse matrix hasil fit transform\n",
    "# menjadi array\n",
    "matrix = matrix.toarray()\n",
    "\n",
    "# perintah menentukan kata-kata unik\n",
    "name = vec.get_feature_names_out()\n",
    "\n",
    "# perintah menggabungkan nama dan nilai matrix dalam data frame\n",
    "tfidf_vector = pd.DataFrame({'Word': name, 'AWS':matrix[0], 'Wikipedia':matrix[1], 'IBM': matrix[2], 'Query':matrix[3]})\n",
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e192cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Query dan AWS: 0.0000\n",
      "Cosine Similarity Query dan Wikipedia: 0.1304\n",
      "Cosine Similarity Query dan IBM: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# membuat fungsi untuk menghitung cosine similarity\n",
    "def count_cosine(query, doc):\n",
    "    xy = [x * y for x, y in zip(query, doc)]\n",
    "    sum_xy = sum(xy)\n",
    "    x2 = [math.pow(x,2) for x in query]\n",
    "    sum_x2 = math.sqrt(sum(x2))\n",
    "    y2 = [math.pow(y,2) for y in doc]\n",
    "    sum_y2 = math.sqrt(sum(y2))\n",
    "    cosine_xy = sum_xy / (sum_x2 * sum_y2)\n",
    "    return cosine_xy\n",
    "\n",
    "# perintah untuk menghitung cosine similarity\n",
    "# antara query dengan dataset\n",
    "# dengan memanggil fungsi count_cosine\n",
    "cosine_Q_dok1 = count_cosine(TF_IDFQ, TF_IDF1)\n",
    "cosine_Q_dok2 = count_cosine(TF_IDFQ, TF_IDF2)\n",
    "cosine_Q_dok3 = count_cosine(TF_IDFQ, TF_IDF3)\n",
    "\n",
    "# perintah untuk menampilkan hasil cosine\n",
    "print(f\"Cosine Similarity Query dan AWS: {cosine_Q_dok1:.4f}\")\n",
    "print(f\"Cosine Similarity Query dan Wikipedia: {cosine_Q_dok2:.4f}\")\n",
    "print(f\"Cosine Similarity Query dan IBM: {cosine_Q_dok3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2817f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity AWS dan Wikipedia: 0.0729\n",
      "Cosine Similarity AWS dan IBM: 0.0863\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk menghitung cosine similarity\n",
    "# antara semua dokumen dalam dataset\n",
    "# selain query\n",
    "# dengan memanggil fungsi count_cosine\n",
    "cosine_Q_dok2 = count_cosine(TF_IDF1, TF_IDF2)\n",
    "cosine_Q_dok3 = count_cosine(TF_IDF1, TF_IDF3)\n",
    "\n",
    "# perintah untuk menampilkan hasil cosine\n",
    "print(f\"Cosine Similarity AWS dan Wikipedia: {cosine_Q_dok2:.4f}\")\n",
    "print(f\"Cosine Similarity AWS dan IBM: {cosine_Q_dok3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17c8e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity AWS dan Wikipedia: 0.0729\n",
      "Cosine Similarity AWS dan IBM: 0.0863\n"
     ]
    }
   ],
   "source": [
    "# perintah untuk menghitung cosine similarity\n",
    "# menggunakan librari python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# perintah untuk mengubah TF_IDF menjadi 2D array (1, n)\n",
    "TF_IDF1 = TF_IDF1.reshape(1, -1)\n",
    "TF_IDF2 = TF_IDF2.reshape(1, -1)\n",
    "TF_IDF3 = TF_IDF3.reshape(1, -1)\n",
    "\n",
    "# perintah menghitung cosine similarity\n",
    "# dengan memanggil function cosine_similarity\n",
    "sim_2 = cosine_similarity(TF_IDF1, TF_IDF2)\n",
    "sim_3 = cosine_similarity(TF_IDF1, TF_IDF3)\n",
    "\n",
    "# perintah untuk menampilkan hasil cosine\n",
    "print(f\"Cosine Similarity AWS dan Wikipedia: {sim_2[0][0]:.4f}\")\n",
    "print(f\"Cosine Similarity AWS dan IBM: {sim_3[0][0]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
